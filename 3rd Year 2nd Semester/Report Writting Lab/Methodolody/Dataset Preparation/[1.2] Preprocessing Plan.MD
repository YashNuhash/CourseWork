
# üìå Expanded **1.2 Preprocessing Plan**

Even though **BanFakeNews-2.0** is ‚Äúcleaned,‚Äù you still need consistent preprocessing to ensure quality input for **psycholinguistic + discourse-aware modeling**.

---

## üîπ 1.2.1 Text Normalization

* **Goal**: Ensure consistency across all Bangla texts.
* **Steps**:

  1. Apply **Unicode Normalization Form C (NFC)** to remove inconsistencies (e.g., ‡¶Ö vs ‡¶Ü ligatures).
  2. Remove unwanted control characters like Zero-Width Non-Joiner (ZWNJ) and Zero-Width Space.
  3. Standardize quotation marks (‚Äú ‚Äù ‚Üí ") and dashes (‚Äì ‚Üí -).
  4. Convert multiple spaces into a single space.
  5. Convert Bangla numerals (‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ) ‚Üí either keep them (if meaningful) or map to English numerals.

‚úÖ **Why**: Bangla often suffers from script inconsistencies across sources.

---

## üîπ 1.2.2 Case Folding (Optional)

* **Bangla script** does not have upper/lower case like English.
* But, news articles may contain **English words** (e.g., "COVID", "Bangladesh").
* **Strategy**: Convert English text to lowercase while keeping Bangla untouched.

---

## üîπ 1.2.3 Noise Removal

* Remove **non-linguistic symbols**:

  * URLs ‚Üí `http://...`, `www...`
  * Email addresses ‚Üí `abc@gmail.com`
  * Mentions ‚Üí `@username`
  * Hashtags ‚Üí `#topic`
  * Numbers (if not critical for meaning).
* Remove **punctuation** (`‡•§ , ? ! - ‚Ä¶ " ' ( ) [ ] { }`).
* Remove **emoji/special symbols** (using regex or Unicode filters).

‚úÖ **Why**: These tokens do not help psycholinguistic or discourse modeling.

---

## üîπ 1.2.4 Sentence Splitting

* Some articles contain **multiple sentences**.
* Use sentence tokenization:

  * Split on `‡•§` (Bangla danda) and `?` or `!`.
  * Tools: IndicNLP or regex-based splitting.

‚úÖ **Why**: Necessary for discourse-level analysis.

---

## üîπ 1.2.5 Word Tokenization

* **Bangla word segmentation** is tricky (compound words, inflections).
* Use:

  * **BNLP** word tokenizer
  * or HuggingFace `bert-base-bengali` tokenizer.
* Example:

  ```
  Text: "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡ßú ‡¶ò‡¶ü‡¶®‡¶æ ‡¶ò‡¶ü‡ßá‡¶õ‡ßá‡•§"
  Tokens: ['‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá', '‡¶Ü‡¶ú‡¶ï‡ßá', '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶¨‡ßú', '‡¶ò‡§ü‡§®‡§æ', '‡¶ò‡¶ü‡ßá‡¶õ‡ßá']
  ```

‚úÖ **Why**: Required for embedding models + feature extraction.

---

## üîπ 1.2.6 Stopword Removal

* Use **Bangla stopword list** (BNLP/IndicNLP).
* Common stopwords: `‡¶Ø‡ßá`, `‡¶Ø‡¶æ`, `‡¶è‡¶¨‡¶Ç`, `‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ`, `‡¶§‡¶æ‡¶π‡¶≤‡ßá`, `‡¶§‡¶¨‡ßá`.
* Remove stopwords **only when doing classical ML (TF-IDF, n-grams)**.
* Keep stopwords **if using Transformers (BERT, LLMs)** because context matters.

‚úÖ **Why**: Stopwords often dilute n-gram models but are important for deep models.

---

## üîπ 1.2.7 Lemmatization / Stemming (Optional)

* Bangla has **rich morphology** (inflections, suffixes).
* Tools: IndicNLP stemmer, rule-based lemmatization.
* Example:

  * ‡¶ò‡¶ü‡ßá‡¶õ‡ßá ‚Üí ‡¶ò‡¶ü
  * ‡¶ñ‡ßá‡¶≤‡¶õ‡ßá ‚Üí ‡¶ñ‡ßá‡¶≤
* Use **lemmatization** if you plan to extract **psycholinguistic features**, but skip for BERT-based embeddings.

---

## üîπ 1.2.8 Handling Code-Mixing (Bangla + English)

* News and social media often contain **Bangla-English mixing**:

  * Example: *"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶Ü‡¶ú football match ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶ø‡¶§ ‡¶π‡¶¨‡ßá‡•§"*
* Strategy:

  1. Detect English tokens (regex `[A-Za-z]+`).
  2. Option A: Translate English words ‚Üí Bangla (using Google Translate API or custom dictionary).
  3. Option B: Keep them but lowercase + normalize.

‚úÖ **Why**: Mixed-language news can mislead classifiers if untreated.

---

## üîπ 1.2.9 Final Text Formatting

* After all steps, ensure:

  * No extra spaces.
  * Only Bangla (and minimal English if unavoidable).
  * Consistent script.

* Example:

Raw text:

```
"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡ßú ‡¶ò‡¶ü‡¶®‡¶æ ‡¶ò‡¶ü‡ßá‡¶õ‡ßá!!!   Visit www.news.com"
```

Processed text:

```
"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶Ü‡¶ú ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡ßú ‡¶ò‡¶ü‡¶®‡¶æ ‡¶ò‡¶ü‡ßá‡¶õ‡ßá"
```

---

## üîπ 1.2.10 Train-Validation-Test Splits

* BanFakeNews-2.0 already provides **70/15/15 split**.
* Just verify **class balance (Fake vs. Real)** is preserved.
* For robustness: keep **external\_test.csv** as an unseen evaluation set.

---

# ‚úÖ Summary of Expanded Preprocessing Pipeline

1. **Normalize Unicode** (NFC, remove ZWNJ, harmonize ligatures).
2. **Case Folding** (English words ‚Üí lowercase).
3. **Noise Removal** (URLs, emails, hashtags, emojis, punctuation).
4. **Sentence Splitting** (needed for discourse).
5. **Word Tokenization** (BNLP or BERT tokenizer).
6. **Stopword Removal** (for ML models, not for transformers).
7. **Lemmatization/Stemming** (optional, for psycholinguistics).
8. **Handle Code-Mixing** (Bangla + English).
9. **Final Cleaning** (normalize spaces, ensure consistency).
10. **Split Dataset** (train/val/test as provided + external test).

---


